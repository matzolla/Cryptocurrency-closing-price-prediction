# -*- coding: utf-8 -*-
"""Crypto_closing_price.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1eDG2OoHZUWMVkak1pWQsGxcFB-Y4NEmp

# My Notebook for
## Cryptocurrency Closing Price Prediction Challenge


Can you predict the closing price for a cryptocurrency? 

The objective of this challenge is to create a machine learning model that will predict closing price for a crypto currency coin.
"""

# Import libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression,BayesianRidge
from sklearn.preprocessing import MinMaxScaler,StandardScaler,RobustScaler,PolynomialFeatures
from google.colab import files
from sklearn.model_selection import KFold
from sklearn.compose import ColumnTransformer
from sklearn import pipeline

from sklearn.feature_selection import RFE
from itertools import combinations_with_replacement

from sklearn.metrics import mean_squared_error
import warnings
warnings.filterwarnings('ignore')

# Load files
train = pd.read_csv('Train.csv')
test_df = pd.read_csv('Test.csv')
ss = pd.read_csv('SampleSubmission.csv')

# check data shapes
train.shape, test_df.shape, ss.shape

train.describe()

#merging the training and testing set
all_data=pd.concat((train,test_df)).reset_index(drop=True)

#I performed variable-wise droping using the function below
def drop_var(data):
  #let test this 

  #create a set of features which are not collinear #########

  data['poly']=data['high']+data['low']*data['galaxy_score']

  data['social_tweets']=data['tweets']*data['volatility']+data['social_score']*data['correlation_rank']

  data['galaxy_tweets']=data['galaxy_score']*data['volatility'] +data['high']

  data['poly_1']=data['low']+data['open']*data['galaxy_score']

  data['poly_2']=data['open']+data['high']*data['galaxy_score']

  data['poly_3']=data['high']+data['open']*data['galaxy_score']

  data['high_low_open']=data['high']*data['open']+data['low']*data['low']

  ###good factors #######

  data['good_fact']=data['volume']+data['tweets']+data['market_cap']

  data= data.drop(columns=['social_volume_24h_rank','percent_change_24h_rank','market_cap_rank','average_sentiment'
  ,'tweet_sentiment_impact4','tweet_sentiment_impact2','tweet_sentiment_impact1','tweet_sentiment4',
  'tweet_sentiment3','tweet_sentiment2','tweet_favorites','tweet_replies','tweet_quotes','tweet_spam','reddit_comments_score',
  'reddit_comments'])
  #rest of the market
  return data

all_data=drop_var(all_data)

#       ['id', 'asset_id', 'open', 'high', 'low', 'volume', 'market_cap',
#       'url_shares', 'unique_url_shares', 'reddit_posts', 'reddit_posts_score',
#       'tweets', 'tweet_followers', 'tweet_retweets', 'tweet_sentiment1',
#       'tweet_sentiment5', 'tweet_sentiment_impact3',
#       'tweet_sentiment_impact5', 'social_score', 'news', 'price_score',
#       'social_impact_score', 'correlation_rank', 'galaxy_score', 'volatility',
#       'volume_24h_rank', 'social_score_24h_rank', 'medium', 'youtube',
#       'social_volume', 'percent_change_24h', 'market_cap_global', 'close']

#the first step is to perform feature interaction
all_cols = all_data.columns.difference(['id', 'close','asset_id','target_class'])
interact = [col for col in all_data[all_cols].columns if all_data[col].corr(all_data['close']) > 0.7]

#I created a feature interaction function
def feature_interact(data,col):

  #creating dataframes
  df1=pd.DataFrame()

  df2=pd.DataFrame()

  df3=pd.DataFrame()

  df4=pd.DataFrame()


  for p in combinations_with_replacement(data[col].columns,2):

    ######## multiplication ##########

    title_1 = p + ("multiply",)
    df1[title_1] = data[col][p[0]]*data[col][p[1]] 

    ####  addition      #######

    title_2= p+ ("add",)
    df2[title_2]=data[col][p[0]]+data[col][p[1]]

    ##### subtraction  ##########

    title_3= p+("subtract",)
    df3[title_3]=data[col][p[0]]-data[col][p[1]]

    ##### divide   #############

    title_4= p+("divide",)
    df4[title_4]=data[col][p[0]]/data[col][p[1]]


  return pd.concat((data,df1,df2,df3,df4),axis=1)

all_data=feature_interact(all_data,interact)

# the division sign put in some infinity  values, so we remove them together with the NAN
all_data.replace([np.inf, -np.inf], np.nan, inplace=True)
all_data.fillna(0, inplace=True)

train_size=train.shape[0]
##now we can split the data
train=all_data[:train_size]
test=all_data[train_size:]

# fill missing values
train = train.fillna(0)
test = test.fillna(0)

test= test.drop('close', axis=1)
all_cols = train.columns.difference(['id', 'close'])

# Select main columns to be used in training
main_cols = train.columns.difference(['id', 'close','asset_id','target_class'])
X = train[main_cols]
y = train.close.astype(float)

col_1=[('high', 'high', 'add'),
       ('high', 'high', 'multiply'),
       ('high', 'low', 'add'),
       ('high', 'low', 'multiply'), ('high', 'low', 'subtract'),
       ('high', 'market_cap', 'add'), ('high', 'market_cap', 'divide'),
       ('high', 'market_cap', 'multiply'),
       ('high', 'market_cap', 'subtract'),
       ('high', 'market_cap_global', 'add'),
       ('high', 'market_cap_global', 'divide'),
       ('high', 'market_cap_global', 'multiply'),
       ('high', 'market_cap_global', 'subtract'), ('high', 'open', 'add'),
       ('high', 'open', 'multiply'),
        ('high', 'poly', 'add'),
       ('high', 'poly', 'multiply'),
       ('high', 'poly', 'subtract'), ('high', 'volume_24h_rank', 'add'),
       ('high', 'volume_24h_rank', 'divide'),
       ('high', 'volume_24h_rank', 'multiply'),
       ('high', 'volume_24h_rank', 'subtract'), ('low', 'low', 'add'),
        ('low', 'low', 'multiply'),
        ('low', 'market_cap', 'add'),
       ('low', 'market_cap', 'divide'), ('low', 'market_cap', 'multiply'),
       ('low', 'market_cap', 'subtract'),
       ('low', 'market_cap_global', 'add'),
       ('low', 'market_cap_global', 'divide'),
       ('low', 'market_cap_global', 'multiply'),
       ('low', 'market_cap_global', 'subtract'), ('low', 'open', 'add'),
       ('low', 'open', 'multiply'),
       ('low', 'poly', 'add'),
       ('low', 'poly', 'multiply'), ('low', 'poly', 'subtract'),
       ('low', 'volume_24h_rank', 'add'),
       ('low', 'volume_24h_rank', 'multiply'),
       ('low', 'volume_24h_rank', 'subtract'),
       ('market_cap', 'market_cap', 'add'),
       ('market_cap', 'market_cap', 'multiply'),
       ('market_cap', 'market_cap_global', 'add'),
       ('market_cap', 'market_cap_global', 'divide'),
       ('market_cap', 'market_cap_global', 'multiply'),
       ('market_cap', 'market_cap_global', 'subtract'),
       ('market_cap', 'open', 'add'), ('market_cap', 'open', 'divide'),
       ('market_cap', 'open', 'multiply'),
       ('market_cap', 'open', 'subtract'), ('market_cap', 'poly', 'add'),
       ('market_cap', 'poly', 'multiply'),
       ('market_cap', 'poly', 'subtract'),
       ('market_cap', 'volume_24h_rank', 'add'),
       ('market_cap', 'volume_24h_rank', 'multiply'),
       ('market_cap', 'volume_24h_rank', 'subtract'),
       ('market_cap_global', 'market_cap_global', 'add'),
       ('market_cap_global', 'open', 'add'),
       ('market_cap_global', 'open', 'multiply'),
       ('market_cap_global', 'open', 'subtract'),
       ('market_cap_global', 'poly', 'add'),
       ('market_cap_global', 'poly', 'subtract'),
       ('market_cap_global', 'volume_24h_rank', 'add'),
       ('market_cap_global', 'volume_24h_rank', 'subtract'),
       ('open', 'open', 'add'),
       ('open', 'open', 'multiply'), ('open', 'poly', 'add'),
        ('open', 'poly', 'multiply'),
       ('open', 'poly', 'subtract'), ('open', 'volume_24h_rank', 'add'),
       ('open', 'volume_24h_rank', 'multiply'),
       ('open', 'volume_24h_rank', 'subtract'), ('poly', 'poly', 'add'),
       ('poly', 'volume_24h_rank', 'add'),
       ('poly', 'volume_24h_rank', 'subtract'),
       ('volume_24h_rank', 'volume_24h_rank', 'add'), 'high', 'low',
       'market_cap', 'market_cap_global', 'open', 'poly',
       'volume_24h_rank','galaxy_tweets','poly_1','poly_2'
       ,'poly_3','high_low_open']

from sklearn.feature_selection import mutual_info_regression

def make_mi_scores(X, y):
    mi_scores = mutual_info_regression(X, y)
    mi_scores = pd.Series(mi_scores, name="MI Scores", index=X.columns)
    mi_scores = mi_scores.sort_values(ascending=False)
    return mi_scores

#mi_score=make_mi_scores(X[col_1],y)

#mi_score[80:]

col_2=[('high', 'high', 'add'),
       ('high', 'high', 'multiply'), ('high', 'low', 'add'),
        ('high', 'low', 'multiply'),
       ('high', 'market_cap', 'add'), 
       ('high', 'market_cap', 'multiply'),
       ('high', 'market_cap', 'subtract'),
       ('high', 'market_cap_global', 'add'),
       ('high', 'market_cap_global', 'multiply'),
       ('high', 'market_cap_global', 'subtract'), ('high', 'open', 'add'),
       ('high', 'open', 'multiply'),
       ('high', 'poly', 'add'),
       ('high', 'poly', 'multiply'), ('high', 'poly', 'subtract'),
       ('high', 'volume_24h_rank', 'add'),
       ('high', 'volume_24h_rank', 'divide'),
       ('high', 'volume_24h_rank', 'multiply'),
       ('high', 'volume_24h_rank', 'subtract'), ('low', 'low', 'add'),
       ('low', 'low', 'multiply'),
       ('low', 'market_cap', 'add'), 
       ('low', 'market_cap', 'multiply'),
       ('low', 'market_cap', 'subtract'),
       ('low', 'market_cap_global', 'add'),
       ('low', 'market_cap_global', 'multiply'),
       ('low', 'market_cap_global', 'subtract'), ('low', 'open', 'add'),
        ('low', 'open', 'multiply'),
       ('low', 'poly', 'add'), 
       ('low', 'poly', 'multiply'), ('low', 'poly', 'subtract'),
       ('low', 'volume_24h_rank', 'add'),
       ('low', 'volume_24h_rank', 'divide'),
       ('low', 'volume_24h_rank', 'multiply'),
       ('low', 'volume_24h_rank', 'subtract'),
       ('market_cap', 'market_cap', 'add'),
       ('market_cap', 'market_cap', 'multiply'),
       ('market_cap', 'market_cap_global', 'add'),
       ('market_cap', 'market_cap_global', 'multiply'),
       ('market_cap', 'market_cap_global', 'subtract'),
       ('market_cap', 'open', 'add'), 
       ('market_cap', 'open', 'multiply'),
       ('market_cap', 'open', 'subtract'), ('market_cap', 'poly', 'add'),
       ('market_cap', 'poly', 'subtract'),
       ('market_cap', 'volume_24h_rank', 'divide'),
       ('market_cap', 'volume_24h_rank', 'multiply'),
       ('market_cap_global', 'market_cap_global', 'add'),
       
       ('market_cap_global', 'market_cap_global', 'multiply'),
       ('market_cap_global', 'open', 'add'),
       ('market_cap_global', 'open', 'divide'),
       ('market_cap_global', 'open', 'multiply'),
       ('market_cap_global', 'open', 'subtract'),
       ('market_cap_global', 'poly', 'add'),
       ('market_cap_global', 'poly', 'multiply'),
       ('market_cap_global', 'poly', 'subtract'),
       ('market_cap_global', 'volume_24h_rank', 'add'),
       ('market_cap_global', 'volume_24h_rank', 'divide'),
       ('market_cap_global', 'volume_24h_rank', 'multiply'),
       ('market_cap_global', 'volume_24h_rank', 'subtract'),
       ('open', 'open', 'add'),
       ('open', 'open', 'multiply'), ('open', 'poly', 'add'),
        ('open', 'poly', 'multiply'),
       ('open', 'poly', 'subtract'), ('open', 'volume_24h_rank', 'add'),
       ('open', 'volume_24h_rank', 'divide'),
       ('open', 'volume_24h_rank', 'multiply'),
       ('open', 'volume_24h_rank', 'subtract'), ('poly', 'poly', 'add'),
        'high', 'low',
       'market_cap', 'market_cap_global', 'open', 'poly','galaxy_tweets','poly_1','poly_2',
       'high_low_open','poly_3',
('poly', 'volume_24h_rank', 'subtract'),
('poly', 'volume_24h_rank', 'add'),         
('high', 'market_cap_global', 'divide'),         
('low', 'market_cap_global', 'divide'),           
('poly', 'volume_24h_rank', 'multiply'),          
('market_cap', 'market_cap_global', 'divide'),    
('poly', 'volume_24h_rank', 'divide'),            
('market_cap_global', 'open', 'divide'),          
('low', 'market_cap', 'divide'),                  
('high', 'market_cap', 'divide'),                 
('market_cap', 'open', 'divide'),                
('low', 'open', 'divide')]

#mi_score=make_mi_scores(X[col_2],y)

#model = RFE(estimator = BayesianRidge(), n_features_to_select = 100)
#model.fit(X_train,y_train)
col=[col_1,col_2]

#now I create a stacking model
Linear_regressors=[LinearRegression(),BayesianRidge()] #

def stack_model(models,X,y,col,test_data,test_df):

  

  #creating a list to append the values of predictions of training and testing in each case

  train_prediction = []
  test_prediction = []
  pred = []
  scores=[]

  #We now decide to fit differrent models

  for i,model in  enumerate(models):


    sc = RobustScaler()

    ## splitting into train and validation
    X_train = X[col[i]].loc[:9500]
    X_test = X[col[i]].loc[9500:]
    y_train = y.loc[:9500]
    y_test = y.loc[9500:]
    test=test_data[col[i]]

    ###preprocessing#############

    sc.fit(X_test)
    X_train = sc.transform(X_train)

    X_test = sc.transform(X_test)
    test = sc.transform(test)


    #fitting the models

    #model = RFE(estimator = model , n_features_to_select = 100)
    model.fit(X_train, y_train)
    train_pred = model.predict(X_train)
    test_pred = model.predict(X_test)
    train_predt = model.predict(test)    
  
    #computing the rmse for both training and testing

    test_rmse= mean_squared_error(y_test, test_pred, squared=False)
    train_rmse = mean_squared_error(y_train, train_pred, squared=False)
    train_prediction.append(train_pred)
    test_prediction.append(test_pred)
    pred.append(train_predt)



    print(model, "train rmse:", train_rmse, "test rmse:", test_rmse)
    scores.append(test_rmse)
  print("Average rmse:",np.mean(scores))

  #now we create an empty dataframe to fit in the predicted output

  submit=pd.DataFrame()
  submit['id'] = test_df['id']
  submit['pred'] = np.mean(pred, axis=0)

  return submit

submit=stack_model(Linear_regressors,X,y,col,test[main_cols],test_df)

submit['pred']=submit['pred'].apply(lambda x: 0 if x<10 else x)

#now we can 
#n=100 41.87497004207674

submit.head()

"""we have noticed that when ever the high and the low price or high and open price or open and low price is thesame, the closing price is the other price, so we used this idea to scale our outputs, by averaging them (It doesn't whole for every trading activity)"""

def kubrah_function(data,exp):

  #the function to compare the predicted close price and open,low and high price

  for i,j in enumerate(exp):

    ##compare low and close

    if exp.loc[i,'close_low']<2:

      data.loc[i,'pred']=exp.loc[i,'low']

    ## compare high and close

    elif exp.loc[i,'close_high']<2:

      data.loc[i,'pred']=exp.loc[i,'high']

    ## compare open and close

    elif exp.loc[i,'open_close']<2:

      data.loc[i,'pred']=exp.loc[i,'open']

  ## return the function ####

  return data

test_df['close']=submit.pred

"""The first step is to create a dataframe containing the high,low,open and predicted close price of the test set. Next I compute the difference between *(high,close), (low,close) and (open,close)*.
1. Base on the previous hypothesis if the difference between, high and close is small then the closing price is probably the high price.
1. If the difference between, low and close is small then the closing price is probably the low price.
1. If the difference between, open and close is small then the closing price is probably the open price.
"""

exp=test_df[['open','high','low','close']]
exp['open_close']=np.abs(exp['open']-exp['close'])
exp['close_high']=np.abs(exp['close']-exp['high'])
exp['close_low']=np.abs(exp['close']-exp['low'])

# use the kubrah_function
submit=kubrah_function(submit,exp)

submit.head()

submit.to_csv('kubrah.csv',index=False)